{
 "cells": [
  {
   "cell_type": "code",
   "id": "096c87a3",
   "metadata": {},
   "outputs": [],
   "source": "# Core imports for NFL schedule parsing\nimport re\nimport pandas as pd\nimport json\nimport pdfplumber\nfrom datetime import datetime"
  },
  {
   "cell_type": "code",
   "id": "6ece60b9",
   "metadata": {},
   "outputs": [],
   "source": "# DEPRECATED: Old PyPDF2 parser - replaced by pdfplumber version\n# This function had issues with mangled text extraction and incorrect dates\n# Left here for reference but use parse_nfl_with_pdfplumber_full instead"
  },
  {
   "cell_type": "code",
   "id": "6a5pdymafnh",
   "metadata": {},
   "outputs": [],
   "source": "import pdfplumber\nimport pandas as pd\nimport re\nimport json\nfrom datetime import datetime\n\ndef parse_nfl_with_pdfplumber_full(pdf_path, season_year):\n    \"\"\"Parse NFL schedule PDF using pdfplumber - full version with URL generation\"\"\"\n    \n    # Load team codes for URL generation\n    with open('teams.json', 'r') as f:\n        team_codes = json.load(f)\n    \n    games = []\n    game_id = 1\n    current_week = 1\n    current_date = None\n    \n    with pdfplumber.open(pdf_path) as pdf:\n        for page_num, page in enumerate(pdf.pages):\n            # Extract text from page\n            text = page.extract_text()\n            if not text:\n                continue\n                \n            lines = text.split('\\n')\n            \n            for line in lines:\n                line = line.strip()\n                if not line:\n                    continue\n                \n                # Check for week headers\n                if 'WEEK' in line and re.search(r'WEEK\\s+\\d+', line):\n                    week_match = re.search(r'WEEK\\s+(\\d+)', line)\n                    if week_match:\n                        current_week = int(week_match.group(1))\n                \n                # Check for date patterns\n                date_match = re.search(r'(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),?\\s*(\\d{4})', line)\n                if date_match:\n                    day_name = date_match.group(1)\n                    month_name = date_match.group(2)\n                    day = int(date_match.group(3))\n                    year = int(date_match.group(4))\n                    \n                    month_map = {\n                        'January': 1, 'February': 2, 'March': 3, 'April': 4,\n                        'May': 5, 'June': 6, 'July': 7, 'August': 8,\n                        'September': 9, 'October': 10, 'November': 11, 'December': 12\n                    }\n                    month = month_map[month_name]\n                    current_date = datetime(year, month, day).date()\n                \n                # Check for games - both \"at\" and \"vs\" (international)\n                elif (' at ' in line or ' vs ' in line) and current_date:\n                    # Skip header lines\n                    if 'GAME' in line and 'LOCAL' in line and 'ET' in line:\n                        continue\n                    \n                    # Split on either \"at\" or \"vs\"\n                    if ' at ' in line:\n                        parts = line.split(' at ')\n                        is_international = False\n                    else:\n                        parts = line.split(' vs ')\n                        is_international = True\n                        \n                    if len(parts) == 2:\n                        away_team = parts[0].strip()\n                        \n                        # Extract home team (before times/other info)\n                        rest = parts[1]\n                        # Split on common patterns to isolate team name\n                        home_parts = re.split(r'\\s+\\d+:\\d+|\\s+\\([^)]*\\)\\s+\\d|\\s+[A-Z]{2,4}\\s*$', rest)\n                        home_team = home_parts[0].strip()\n                        \n                        # Clean up team names\n                        home_team = re.sub(r'\\s+\\([^)]*\\)$', '', home_team)  # Remove trailing parentheses\n                        \n                        if away_team and home_team and len(away_team) > 3 and len(home_team) > 3:\n                            # Generate PFR URL\n                            home_code = team_codes.get(home_team)\n                            away_code = team_codes.get(away_team)\n                            \n                            pfr_url = None\n                            if home_code and current_date:\n                                date_str = current_date.strftime('%Y%m%d')\n                                pfr_url = f\"https://www.pro-football-reference.com/boxscores/{date_str}0{home_code}.htm\"\n                            \n                            # Extract additional details\n                            day_match = re.search(r'\\((Thu|Mon|Tue|Wed|Fri|Sat|Sun)\\)', line)\n                            day_of_week = day_match.group(1) if day_match else None\n                            \n                            times = re.findall(r'\\d+:\\d+p', line)\n                            local_time = times[0] if times else None\n                            et_time = times[1] if len(times) > 1 else times[0] if times else None\n                            \n                            networks = ['NBC', 'CBS', 'FOX', 'ESPN', 'NFLN', 'Prime Video', 'Peacock', 'ESPN/ABC']\n                            tv_network = None\n                            for network in networks:\n                                if network in line:\n                                    tv_network = network\n                                    break\n                            \n                            games.append({\n                                'game_id': f\"{season_year}_{game_id:03d}\",\n                                'season': season_year,\n                                'week': current_week,\n                                'game_date': current_date,\n                                'day_of_week': day_of_week,\n                                'away_team': away_team,\n                                'home_team': home_team,\n                                'local_time': local_time,\n                                'et_time': et_time,\n                                'tv_network': tv_network,\n                                'pfr_url': pfr_url,\n                                'pfr_home_code': home_code,\n                                'pfr_away_code': away_code,\n                                'is_international': is_international\n                            })\n                            game_id += 1\n    \n    return pd.DataFrame(games)\n\n# Parse the full 2024 season\nprint(\"Parsing full 2024 NFL schedule with pdfplumber...\")\ngames_2024_full = parse_nfl_with_pdfplumber_full('NFL-Regular-season-2024.pdf', 2024)\n\nprint(f\"Found {len(games_2024_full)} games total\")\n\n# Check for international games\ninternational_games = games_2024_full[games_2024_full['is_international'] == True]\nprint(f\"International games (vs): {len(international_games)}\")\n\nprint(\"\\nFirst 5 games:\")\nprint(games_2024_full[['game_id', 'week', 'game_date', 'away_team', 'home_team', 'is_international']].head(5))\n\nprint(\"\\nLast 5 games:\")\nprint(games_2024_full[['game_id', 'week', 'game_date', 'away_team', 'home_team', 'is_international']].tail(5))\n\n# Save to CSV for batch scraper\ncsv_filename = 'nfl_2024_schedule.csv'\ngames_2024_full.to_csv(csv_filename, index=False)\nprint(f\"\\nðŸ“„ Saved {len(games_2024_full)} games to {csv_filename} for batch scraper\")\n\n# Also save corrected version for auditing\ngames_2024_full.to_csv('nfl_2024_schedule_corrected.csv', index=False)\nprint(f\"ðŸ“„ Saved audit copy to nfl_2024_schedule_corrected.csv\")"
  },
  {
   "cell_type": "markdown",
   "id": "fcy8sqc9fgc",
   "metadata": {},
   "source": [
    "# NFL Player Stats Scraper\n",
    "\n",
    "Test scraping player statistics from Pro Football Reference box scores."
   ]
  },
  {
   "cell_type": "code",
   "id": "hvcqq49qlnc",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for testing single game scraping\n# This was used during development but no longer needed"
  },
  {
   "cell_type": "code",
   "id": "2pv99tdlpfe",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for parsing table structure\n# This was used during development but no longer needed"
  },
  {
   "cell_type": "code",
   "id": "62gxo3ragd5",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for finding advanced tables in HTML comments\n# This was used during development but no longer needed"
  },
  {
   "cell_type": "code",
   "id": "zzk2hkhk68o",
   "metadata": {},
   "outputs": [],
   "source": "# This cell removed - was just testing single game scraping and creating unnecessary files"
  },
  {
   "cell_type": "markdown",
   "id": "0aanrnpozeq",
   "metadata": {},
   "source": [
    "# Batch NFL Stats Scraper\n",
    "\n",
    "Scrape all 272 games from 2024 season in random order with delays."
   ]
  },
  {
   "cell_type": "code",
   "id": "yqv8iqm44w",
   "metadata": {},
   "outputs": [],
   "source": "import random\nimport time\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\n\ndef scrape_game_tables_with_id(soup, game_id, game_url):\n    \"\"\"Extract all 4 tables and add game_id to each record\"\"\"\n    \n    tables = {}\n    \n    # 1. Basic offense table  \n    basic_table = soup.find('table', {'id': 'player_offense'})\n    if basic_table:\n        basic_data = []\n        tbody = basic_table.find('tbody')\n        if tbody:\n            rows = tbody.find_all('tr')\n            \n            for row in rows:\n                if row.get('class') and ('thead' in str(row.get('class')) or 'over_header' in str(row.get('class'))):\n                    continue\n                    \n                cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]\n                \n                if cells and cells[0] == 'Player':\n                    continue\n                    \n                if len(cells) > 1 and cells[0]:\n                    # Add game_id as first column\n                    row_data = [game_id] + cells\n                    basic_data.append(row_data)\n        \n        if basic_data:\n            columns = ['game_id', 'Player', 'Tm', 'Pass_Cmp', 'Pass_Att', 'Pass_Yds', 'Pass_TD', 'Pass_Int', \n                      'Pass_Sk', 'Pass_Sk_Yds', 'Pass_Lng', 'Pass_Rate', 'Rush_Att', 'Rush_Yds', \n                      'Rush_TD', 'Rush_Lng', 'Rec_Tgt', 'Rec_Rec', 'Rec_Yds', 'Rec_TD', 'Rec_Lng', \n                      'Fmb', 'FL']\n            while len(columns) < len(basic_data[0]):\n                columns.append(f'Col_{len(columns)}')\n            \n            tables['basic_offense'] = pd.DataFrame(basic_data, columns=columns[:len(basic_data[0])])\n    \n    # 2-4. Advanced tables from comments\n    from bs4 import Comment\n    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n    \n    advanced_sections = {\n        'advanced_passing': 'Advanced Passing',\n        'advanced_rushing': 'Advanced Rushing', \n        'advanced_receiving': 'Advanced Receiving'\n    }\n    \n    for table_name, section_text in advanced_sections.items():\n        for comment in comments:\n            if section_text.lower() in comment.lower():\n                comment_soup = BeautifulSoup(comment, 'html.parser')\n                adv_table = comment_soup.find('table')\n                \n                if adv_table:\n                    headers = ['game_id']  # Start with game_id\n                    header_row = adv_table.find('thead') or adv_table.find('tr')\n                    if header_row:\n                        for th in header_row.find_all(['th', 'td']):\n                            headers.append(th.get_text().strip())\n                    \n                    data_rows = []\n                    rows = adv_table.find_all('tr')[1:]\n                    \n                    for row in rows:\n                        if row.get('class') and ('thead' in str(row.get('class')) or 'over_header' in str(row.get('class'))):\n                            continue\n                            \n                        cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]\n                        \n                        if cells and cells[0] == 'Player':\n                            continue\n                            \n                        if len(cells) > 1 and cells[0]:\n                            # Add game_id as first column\n                            row_data = [game_id] + cells\n                            data_rows.append(row_data)\n                    \n                    if data_rows and headers:\n                        max_cols = max(len(headers), len(data_rows[0]) if data_rows else 0)\n                        while len(headers) < max_cols:\n                            headers.append(f'Col_{len(headers)}')\n                        \n                        tables[table_name] = pd.DataFrame(data_rows, columns=headers[:len(data_rows[0])])\n                break\n    \n    return tables\n\ndef log_error(message, log_file=\"scraper_errors.log\"):\n    \"\"\"Log errors to file with timestamp\"\"\"\n    with open(log_file, 'a') as f:\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        f.write(f\"[{timestamp}] {message}\\n\")\n\ndef format_time_remaining(seconds):\n    \"\"\"Format seconds into readable time string\"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    if hours > 0:\n        return f\"{hours}h {minutes}m\"\n    else:\n        return f\"{minutes}m\"\n\ndef load_nfl_facts():\n    \"\"\"Load NFL facts from facts.txt file\"\"\"\n    try:\n        with open('facts.txt', 'r') as f:\n            content = f.read()\n        \n        # Split by lines and filter out empty lines\n        facts = [line.strip() for line in content.split('\\n') if line.strip() and not line.strip().isdigit()]\n        return facts\n    except FileNotFoundError:\n        return [\"Did you know? The NFL was founded in 1920!\"]  # Fallback fact\n\ndef batch_scrape_season():\n    \"\"\"\n    WARNING: This will scrape ALL 272 games from 2024 season!\n    Estimated time: 2-3 hours with 15-45 second delays\n    \"\"\"\n    \n    print(\"BATCH NFL SCRAPER STARTING\")\n    print(\"This will take 2-3 hours to complete all 272 games\")\n    \n    # Clean up previous runs\n    files_to_remove = [\"scraper_errors.log\"] + [f for f in os.listdir('.') if f.startswith('clean_game_')]\n    for file in files_to_remove:\n        if os.path.exists(file):\n            os.remove(file)\n    \n    if os.path.exists(\"nfl_2024_data\"):\n        shutil.rmtree(\"nfl_2024_data\")\n        print(\"Cleaned up previous run data\")\n    \n    # Load NFL facts for entertainment\n    nfl_facts = load_nfl_facts()\n    \n    # Create output directory\n    output_dir = \"nfl_2024_data\"\n    os.makedirs(output_dir)\n    print(f\"Created output directory: {output_dir}\")\n    \n    # Initialize error log\n    log_error(\"=== BATCH SCRAPING SESSION STARTED ===\")\n    \n    start_time = datetime.now()\n    print(f\"\\nSCRAPING STARTED at {start_time.strftime('%H:%M:%S')}\")\n    \n    # Load schedule\n    schedule_df = pd.read_csv('nfl_2024_schedule.csv')\n    total_games = len(schedule_df)\n    print(f\"Loaded {total_games} games to scrape\")\n    \n    # Calculate time estimates\n    avg_delay = (15 + 45) / 2  # 30 seconds average\n    estimated_total_seconds = total_games * avg_delay\n    estimated_hours = estimated_total_seconds / 3600\n    estimated_end = start_time + timedelta(seconds=estimated_total_seconds)\n    \n    print(f\"Estimated completion: {estimated_end.strftime('%H:%M:%S')} ({estimated_hours:.1f} hours)\")\n    \n    # Randomize order\n    games_to_scrape = schedule_df.sample(frac=1).reset_index(drop=True)\n    print(\"Shuffled games randomly\")\n    \n    # Initialize master DataFrames\n    master_tables = {\n        'basic_offense': [],\n        'advanced_passing': [],\n        'advanced_rushing': [],\n        'advanced_receiving': []\n    }\n    \n    successful_scrapes = 0\n    failed_scrapes = 0\n    \n    for i, game in games_to_scrape.iterrows():\n        game_id = game['game_id']\n        url = game['pfr_url']\n        \n        # Progress calculation\n        progress_pct = (i / total_games) * 100\n        games_remaining = total_games - i - 1\n        elapsed = datetime.now() - start_time\n        \n        print(f\"\\n[{i+1}/{total_games}] ({progress_pct:.1f}%) Scraping {game_id}\")\n        print(f\"Game: {game['away_team']} @ {game['home_team']}\")\n        print(f\"URL: {url}\")\n        \n        try:\n            # Make request\n            response = requests.get(url)\n            \n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, 'html.parser')\n                game_tables = scrape_game_tables_with_id(soup, game_id, url)\n                \n                # Check if we got any data\n                total_players = sum(len(df) for df in game_tables.values() if not df.empty)\n                \n                if total_players == 0:\n                    # No data found - log as potential date/parsing issue\n                    error_msg = f\"NO DATA FOUND - {game_id} | {url} | Teams: {game['away_team']} @ {game['home_team']} | Date: {game.get('game_date', 'Unknown')}\"\n                    log_error(error_msg)\n                    print(f\"  WARNING: No player data found - logged to errors\")\n                else:\n                    # Accumulate results\n                    table_counts = {}\n                    for table_name, df in game_tables.items():\n                        if not df.empty:\n                            master_tables[table_name].append(df)\n                            table_counts[table_name] = len(df)\n                        else:\n                            table_counts[table_name] = 0\n                    \n                    # Display table counts - more concise\n                    total_records = sum(table_counts.values())\n                    print(f\"  Data: {total_records} total records ({', '.join([f'{k}: {v}' for k, v in table_counts.items()])})\")\n                \n                successful_scrapes += 1\n                success_rate = (successful_scrapes / (successful_scrapes + failed_scrapes)) * 100\n                print(f\"  SUCCESS ({successful_scrapes}/{successful_scrapes+failed_scrapes}) {success_rate:.1f}%\")\n                \n            elif response.status_code == 404:\n                # Try alternate URL with other team's code\n                print(f\"  404 on primary URL, trying alternate...\")\n                \n                # Create alternate URL with away team code - handle both string and datetime\n                if isinstance(game['game_date'], str):\n                    date_str = game['game_date'].replace('-', '')  # Convert 2024-12-08 to 20241208\n                else:\n                    date_str = game['game_date'].strftime('%Y%m%d') if pd.notnull(game.get('game_date')) else 'UNKNOWN'\n                \n                alt_url = f\"https://www.pro-football-reference.com/boxscores/{date_str}0{game['pfr_away_code']}.htm\"\n                print(f\"  Alt URL: {alt_url}\")\n                \n                alt_response = requests.get(alt_url)\n                \n                if alt_response.status_code == 200:\n                    print(f\"  SUCCESS with alternate URL\")\n                    soup = BeautifulSoup(alt_response.content, 'html.parser')\n                    game_tables = scrape_game_tables_with_id(soup, game_id, alt_url)\n                    \n                    # Check if we got any data\n                    total_players = sum(len(df) for df in game_tables.values() if not df.empty)\n                    \n                    if total_players == 0:\n                        error_msg = f\"NO DATA FOUND - {game_id} | {alt_url} | Teams: {game['away_team']} @ {game['home_team']} | Date: {game.get('game_date', 'Unknown')}\"\n                        log_error(error_msg)\n                        print(f\"  WARNING: No player data found - logged to errors\")\n                    else:\n                        # Accumulate results\n                        table_counts = {}\n                        for table_name, df in game_tables.items():\n                            if not df.empty:\n                                master_tables[table_name].append(df)\n                                table_counts[table_name] = len(df)\n                            else:\n                                table_counts[table_name] = 0\n                        \n                        # Display table counts - more concise\n                        total_records = sum(table_counts.values())\n                        print(f\"  Data: {total_records} total records ({', '.join([f'{k}: {v}' for k, v in table_counts.items()])})\")\n                    \n                    successful_scrapes += 1\n                    success_rate = (successful_scrapes / (successful_scrapes + failed_scrapes)) * 100\n                    print(f\"  SUCCESS ({successful_scrapes}/{successful_scrapes+failed_scrapes}) {success_rate:.1f}%\")\n                else:\n                    # Both URLs failed - log error\n                    error_msg = f\"404 BOTH URLs - {game_id} | Primary: {url} | Alternate: {alt_url} | Teams: {game['away_team']} @ {game['home_team']} | Date: {game.get('game_date', 'Unknown')}\"\n                    log_error(error_msg)\n                    print(f\"  ERROR: Both URLs 404'd - logged to errors\")\n                    failed_scrapes += 1\n                \n            else:\n                # Other HTTP error\n                error_msg = f\"HTTP {response.status_code} - {game_id} | {url}\"\n                log_error(error_msg)\n                print(f\"  ERROR: HTTP {response.status_code}\")\n                failed_scrapes += 1\n                \n        except Exception as e:\n            error_msg = f\"EXCEPTION - {game_id} | {url} | Error: {str(e)}\"\n            log_error(error_msg)\n            print(f\"  ERROR: {e}\")\n            failed_scrapes += 1\n        \n        # Calculate time remaining - simplified\n        if i > 0:\n            avg_time_per_game = elapsed.total_seconds() / (i + 1)\n            estimated_remaining_seconds = games_remaining * avg_time_per_game\n            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_seconds)\n            \n            print(f\"  ETA: {estimated_completion.strftime('%H:%M:%S')} ({format_time_remaining(estimated_remaining_seconds)} remaining)\")\n        \n        # Random delay between requests with live countdown and random NFL fact\n        if i < len(games_to_scrape) - 1:  # Don't delay after last game\n            delay = random.randint(15, 45)\n            \n            # Show random NFL fact with subtle formatting\n            random_fact = random.choice(nfl_facts)\n            print(f\"\\nNFL FACT: {random_fact}\\n\")\n            \n            for countdown in range(delay, 0, -1):\n                print(f\"\\r  Waiting {countdown} seconds...\", end=\"\", flush=True)\n                time.sleep(1)\n            print()  # New line after countdown\n    \n    # Combine and save all DataFrames\n    total_time = datetime.now() - start_time\n    print(f\"\\n=== COMBINING RESULTS ===\")\n    print(f\"Total scraping time: {str(total_time).split('.')[0]}\")\n    \n    final_tables = {}\n    \n    for table_name, df_list in master_tables.items():\n        if df_list:\n            combined_df = pd.concat(df_list, ignore_index=True)\n            final_tables[table_name] = combined_df\n            \n            # Save to protected subdirectory\n            filename = os.path.join(output_dir, f\"2024_season_{table_name}.csv\")\n            combined_df.to_csv(filename, index=False)\n            print(f\"SAVED: {table_name}: {len(combined_df)} records -> {filename}\")\n        else:\n            print(f\"ERROR: {table_name}: No data collected\")\n    \n    # Save backup copies with timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    backup_dir = os.path.join(output_dir, f\"backup_{timestamp}\")\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    for table_name, df in final_tables.items():\n        backup_filename = os.path.join(backup_dir, f\"2024_season_{table_name}_backup.csv\")\n        df.to_csv(backup_filename, index=False)\n    \n    print(f\"BACKUP: All files backed up to {backup_dir}\")\n    \n    # Final logging\n    final_msg = f\"SCRAPING COMPLETE - Success: {successful_scrapes} | Failed: {failed_scrapes} | Total time: {str(total_time).split('.')[0]}\"\n    log_error(final_msg)\n    \n    print(f\"\\nSCRAPING COMPLETE!\")\n    print(f\"Successful: {successful_scrapes}\")\n    print(f\"Failed: {failed_scrapes}\")\n    print(f\"Success rate: {successful_scrapes/(successful_scrapes+failed_scrapes)*100:.1f}%\")\n    print(f\"Total time: {str(total_time).split('.')[0]}\")\n    print(f\"Errors logged to: scraper_errors.log\")\n    print(f\"Data saved to: {output_dir}\")\n    \n    return final_tables\n\n# DANGER ZONE: Uncomment to run full season scrape\n# batch_scrape_season()"
  },
  {
   "cell_type": "code",
   "id": "ih9ugf8zook",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for parser troubleshooting\n# This was used during development but no longer needed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}