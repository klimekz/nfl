{
 "cells": [
  {
   "cell_type": "code",
   "id": "8xdshi9ubu8",
   "source": "# Example 3: Analyze specific game performance\nprint(\"=== EXAMPLE: WEEK 1 RAVENS @ CHIEFS GAME ANALYSIS ===\")\n\n# Analyze the season opener\ngame_analysis = analyze_game_performance('2024_001', tables, schedule_df)\n\nif 'error' not in game_analysis:\n    game_info = game_analysis['game_info']\n    print(f\"Game: {game_info['away_team']} @ {game_info['home_team']}\")\n    print(f\"Date: {game_info['game_date']}\")\n    print(f\"Week: {game_info['week']}\")\n    print(f\"TV: {game_info['tv_network']}\")\n    \n    print(f\"\\nPlayer Performances:\")\n    for table_name, players in game_analysis['player_performances'].items():\n        print(f\"\\n{table_name.upper()} ({len(players)} players):\")\n        for player in players[:3]:  # Show top 3 players per table\n            print(f\"  {player.get('Player', 'Unknown')} ({player.get('Tm', 'Unknown')})\")\n            \n    print(f\"\\nTeam Totals:\")\n    for table_name, team_stats in game_analysis['team_totals'].items():\n        print(f\"\\n{table_name.upper()}:\")\n        for team, stats in team_stats.items():\n            key_stats = {k: v for k, v in stats.items() if k in ['Pass_Yds', 'Rush_Yds', 'Pass_TD', 'Rush_TD'] and v > 0}\n            if key_stats:\n                print(f\"  {team}: {key_stats}\")\nelse:\n    print(f\"Error: {game_analysis['error']}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "im0h8ikah2b",
   "source": "# Example 2: Create detailed player profile\nprint(\"=== EXAMPLE: LAMAR JACKSON PLAYER PROFILE ===\")\n\n# Create comprehensive profile for a specific player\nlamar_profile = create_player_profile(\"Lamar Jackson\", tables, schedule_df)\n\nprint(f\"Player: {lamar_profile['player_name']}\")\nprint(f\"Season: {lamar_profile['season']}\")\nprint(f\"Games Played: {lamar_profile['summary']['games_played']}\")\nprint(f\"Teams: {', '.join(lamar_profile['summary']['teams'])}\")\nprint(f\"Weeks Active: {lamar_profile['summary']['weeks_active']}\")\n\nprint(f\"\\nGame Log (first 5 games):\")\nfor i, game in enumerate(lamar_profile['game_log'][:5]):\n    print(f\"  Week {game['week']}: {game['team']} vs {game['opponent']} ({game['home_away']})\")\n    print(f\"    Table: {game['table_type']}\")\n    if game['stats']:\n        key_stats = {k: v for k, v in game['stats'].items() if k in ['Pass_Yds', 'Rush_Yds', 'Pass_TD', 'Rush_TD']}\n        print(f\"    Stats: {key_stats}\")\n    print()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "86xdd3x13xd",
   "source": "# Example 1: Get top rushing yards leaders\nprint(\"=== TOP 10 RUSHING YARDS LEADERS ===\")\ntop_rushers = get_top_players_by_stat('Rush_Yds', 'basic_offense', tables, limit=10)\nif not top_rushers.empty:\n    print(top_rushers[['Player', 'Rush_Yds', 'games_played', 'per_game']].to_string(index=False))\nelse:\n    print(\"No rushing data found\")\n\nprint(\"\\n=== TOP 10 PASSING YARDS LEADERS ===\")\ntop_passers = get_top_players_by_stat('Pass_Yds', 'basic_offense', tables, limit=10)\nif not top_passers.empty:\n    print(top_passers[['Player', 'Pass_Yds', 'games_played', 'per_game']].to_string(index=False))\n\nprint(\"\\n=== TOP 10 RECEIVING YARDS LEADERS ===\")\ntop_receivers = get_top_players_by_stat('Rec_Yds', 'basic_offense', tables, limit=10)\nif not top_receivers.empty:\n    print(top_receivers[['Player', 'Rec_Yds', 'games_played', 'per_game']].to_string(index=False))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hp3bc3pzy85",
   "source": "# Example Usage - Player Analysis Interface\n\nDemonstrate the player aggregation functions with example queries.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7fvysr7nmcd",
   "source": "import pandas as pd\nimport numpy as np\nimport os\nfrom datetime import datetime\n\ndef load_all_game_data():\n    \"\"\"Load all 4 statistical tables into a dictionary\"\"\"\n    data_dir = \"nfl_2024_data\"\n    tables = {}\n    \n    table_files = {\n        'basic_offense': f'{data_dir}/2024_season_basic_offense.csv',\n        'advanced_passing': f'{data_dir}/2024_season_advanced_passing.csv',\n        'advanced_rushing': f'{data_dir}/2024_season_advanced_rushing.csv',\n        'advanced_receiving': f'{data_dir}/2024_season_advanced_receiving.csv'\n    }\n    \n    for table_name, filepath in table_files.items():\n        if os.path.exists(filepath):\n            tables[table_name] = pd.read_csv(filepath)\n            print(f\"Loaded {table_name}: {len(tables[table_name])} records\")\n        else:\n            print(f\"WARNING: {filepath} not found\")\n            tables[table_name] = pd.DataFrame()\n    \n    return tables\n\ndef get_player_season_stats(player_name, tables):\n    \"\"\"Get complete season statistics for a specific player\"\"\"\n    \n    player_stats = {\n        'player_name': player_name,\n        'games_played': 0,\n        'teams': set(),\n        'positions': set(),\n        'game_log': [],\n        'season_totals': {},\n        'averages': {},\n        'advanced_metrics': {}\n    }\n    \n    # Process each table type\n    for table_name, df in tables.items():\n        if df.empty:\n            continue\n            \n        # Find player records\n        player_games = df[df['Player'].str.contains(player_name, case=False, na=False)]\n        \n        if not player_games.empty:\n            player_stats['games_played'] = max(player_stats['games_played'], len(player_games))\n            \n            # Collect team and position info\n            if 'Tm' in player_games.columns:\n                player_stats['teams'].update(player_games['Tm'].dropna().unique())\n            \n            # Add game-by-game data\n            for _, game in player_games.iterrows():\n                game_data = {\n                    'game_id': game.get('game_id', 'Unknown'),\n                    'table_type': table_name,\n                    'team': game.get('Tm', 'Unknown'),\n                    'stats': game.to_dict()\n                }\n                player_stats['game_log'].append(game_data)\n    \n    # Convert sets to lists for JSON serialization\n    player_stats['teams'] = list(player_stats['teams'])\n    player_stats['positions'] = list(player_stats['positions'])\n    \n    return player_stats\n\ndef create_player_profile(player_name, tables, schedule_df=None):\n    \"\"\"Create comprehensive player profile with game context\"\"\"\n    \n    # Load schedule data if not provided\n    if schedule_df is None:\n        schedule_df = pd.read_csv('nfl_2024_schedule.csv')\n    \n    profile = {\n        'player_name': player_name,\n        'season': 2024,\n        'last_updated': datetime.now().isoformat(),\n        'summary': {},\n        'game_log': [],\n        'season_totals': {},\n        'averages': {}\n    }\n    \n    # Collect all game appearances\n    all_games = []\n    \n    for table_name, df in tables.items():\n        if df.empty:\n            continue\n            \n        player_games = df[df['Player'].str.contains(player_name, case=False, na=False)]\n        \n        for _, game in player_games.iterrows():\n            game_id = game.get('game_id', 'Unknown')\n            \n            # Get game context from schedule\n            game_info = schedule_df[schedule_df['game_id'] == game_id]\n            \n            game_record = {\n                'game_id': game_id,\n                'table_type': table_name,\n                'team': game.get('Tm', 'Unknown'),\n                'week': game_info['week'].iloc[0] if not game_info.empty else None,\n                'date': game_info['game_date'].iloc[0] if not game_info.empty else None,\n                'opponent': None,\n                'home_away': None,\n                'stats': {k: v for k, v in game.to_dict().items() if k not in ['game_id', 'Player', 'Tm']}\n            }\n            \n            # Determine opponent and home/away status\n            if not game_info.empty:\n                row = game_info.iloc[0]\n                player_team = game.get('Tm', 'Unknown')\n                \n                if player_team == row['home_team']:\n                    game_record['opponent'] = row['away_team']\n                    game_record['home_away'] = 'home'\n                elif player_team == row['away_team']:\n                    game_record['opponent'] = row['home_team'] \n                    game_record['home_away'] = 'away'\n            \n            all_games.append(game_record)\n    \n    # Sort games by week\n    all_games.sort(key=lambda x: x.get('week', 999))\n    profile['game_log'] = all_games\n    \n    # Calculate summary stats\n    profile['summary']['games_played'] = len(set(game['game_id'] for game in all_games))\n    profile['summary']['teams'] = list(set(game['team'] for game in all_games))\n    profile['summary']['weeks_active'] = len(set(game['week'] for game in all_games if game['week']))\n    \n    return profile\n\ndef get_top_players_by_stat(stat_column, table_name, tables, limit=10):\n    \"\"\"Get top players by a specific statistic\"\"\"\n    \n    if table_name not in tables or tables[table_name].empty:\n        return pd.DataFrame()\n    \n    df = tables[table_name]\n    \n    if stat_column not in df.columns:\n        return pd.DataFrame()\n    \n    # Convert to numeric and aggregate by player\n    df_numeric = df.copy()\n    df_numeric[stat_column] = pd.to_numeric(df_numeric[stat_column], errors='coerce')\n    \n    # Group by player and sum the statistic\n    player_totals = df_numeric.groupby('Player')[stat_column].sum().reset_index()\n    player_totals = player_totals.sort_values(stat_column, ascending=False).head(limit)\n    \n    # Add games played\n    games_played = df_numeric.groupby('Player').size().reset_index(name='games_played')\n    player_totals = player_totals.merge(games_played, on='Player')\n    \n    # Calculate per-game average\n    player_totals['per_game'] = player_totals[stat_column] / player_totals['games_played']\n    \n    return player_totals\n\ndef analyze_game_performance(game_id, tables, schedule_df=None):\n    \"\"\"Analyze all player performances in a specific game\"\"\"\n    \n    if schedule_df is None:\n        schedule_df = pd.read_csv('nfl_2024_schedule.csv')\n    \n    # Get game info\n    game_info = schedule_df[schedule_df['game_id'] == game_id]\n    \n    if game_info.empty:\n        return {'error': f'Game {game_id} not found'}\n    \n    game_context = game_info.iloc[0].to_dict()\n    \n    analysis = {\n        'game_id': game_id,\n        'game_info': game_context,\n        'player_performances': {},\n        'team_totals': {},\n        'top_performers': {}\n    }\n    \n    # Collect all player data for this game\n    for table_name, df in tables.items():\n        if df.empty:\n            continue\n            \n        game_players = df[df['game_id'] == game_id]\n        \n        if not game_players.empty:\n            analysis['player_performances'][table_name] = game_players.to_dict('records')\n            \n            # Calculate team totals for numeric columns\n            numeric_cols = game_players.select_dtypes(include=[np.number]).columns\n            team_totals = {}\n            \n            for team in game_players['Tm'].unique():\n                if pd.notna(team):\n                    team_data = game_players[game_players['Tm'] == team]\n                    team_totals[team] = team_data[numeric_cols].sum().to_dict()\n            \n            analysis['team_totals'][table_name] = team_totals\n    \n    return analysis\n\n# Load all data for analysis\nprint(\"Loading NFL 2024 player data...\")\ntables = load_all_game_data()\n\n# Load schedule for game context\nschedule_df = pd.read_csv('nfl_2024_schedule.csv')\nprint(f\"Loaded schedule: {len(schedule_df)} games\")\n\nprint(\"\\nPlayer aggregation functions ready!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c8qih7p9pll",
   "source": "# Player Aggregation Functions\n\nCreate comprehensive player profiles and season statistics from game-level data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "096c87a3",
   "metadata": {},
   "outputs": [],
   "source": "# Core imports for NFL schedule parsing\nimport re\nimport pandas as pd\nimport json\nimport pdfplumber\nfrom datetime import datetime"
  },
  {
   "cell_type": "code",
   "id": "6ece60b9",
   "metadata": {},
   "outputs": [],
   "source": "# DEPRECATED: Old PyPDF2 parser - replaced by pdfplumber version\n# This function had issues with mangled text extraction and incorrect dates\n# Left here for reference but use parse_nfl_with_pdfplumber_full instead"
  },
  {
   "cell_type": "code",
   "id": "6a5pdymafnh",
   "metadata": {},
   "outputs": [],
   "source": "import pdfplumber\nimport pandas as pd\nimport re\nimport json\nfrom datetime import datetime\n\ndef parse_nfl_with_pdfplumber_full(pdf_path, season_year):\n    \"\"\"Parse NFL schedule PDF using pdfplumber - full version with URL generation\"\"\"\n    \n    # Load team codes for URL generation\n    with open('teams.json', 'r') as f:\n        team_codes = json.load(f)\n    \n    games = []\n    game_id = 1\n    current_week = 1\n    current_date = None\n    \n    with pdfplumber.open(pdf_path) as pdf:\n        for page_num, page in enumerate(pdf.pages):\n            # Extract text from page\n            text = page.extract_text()\n            if not text:\n                continue\n                \n            lines = text.split('\\n')\n            \n            for line in lines:\n                line = line.strip()\n                if not line:\n                    continue\n                \n                # Check for week headers\n                if 'WEEK' in line and re.search(r'WEEK\\s+\\d+', line):\n                    week_match = re.search(r'WEEK\\s+(\\d+)', line)\n                    if week_match:\n                        current_week = int(week_match.group(1))\n                \n                # Check for date patterns\n                date_match = re.search(r'(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),?\\s*(\\d{4})', line)\n                if date_match:\n                    day_name = date_match.group(1)\n                    month_name = date_match.group(2)\n                    day = int(date_match.group(3))\n                    year = int(date_match.group(4))\n                    \n                    month_map = {\n                        'January': 1, 'February': 2, 'March': 3, 'April': 4,\n                        'May': 5, 'June': 6, 'July': 7, 'August': 8,\n                        'September': 9, 'October': 10, 'November': 11, 'December': 12\n                    }\n                    month = month_map[month_name]\n                    current_date = datetime(year, month, day).date()\n                \n                # Check for games - both \"at\" and \"vs\" (international)\n                elif (' at ' in line or ' vs ' in line) and current_date:\n                    # Skip header lines\n                    if 'GAME' in line and 'LOCAL' in line and 'ET' in line:\n                        continue\n                    \n                    # Split on either \"at\" or \"vs\"\n                    if ' at ' in line:\n                        parts = line.split(' at ')\n                        is_international = False\n                    else:\n                        parts = line.split(' vs ')\n                        is_international = True\n                        \n                    if len(parts) == 2:\n                        away_team = parts[0].strip()\n                        \n                        # Extract home team (before times/other info)\n                        rest = parts[1]\n                        # Split on common patterns to isolate team name\n                        home_parts = re.split(r'\\s+\\d+:\\d+|\\s+\\([^)]*\\)\\s+\\d|\\s+[A-Z]{2,4}\\s*$', rest)\n                        home_team = home_parts[0].strip()\n                        \n                        # Clean up team names - REMOVE TBD suffixes\n                        away_team = re.sub(r'\\s+TBD.*$', '', away_team).strip()\n                        home_team = re.sub(r'\\s+TBD.*$', '', home_team).strip()\n                        home_team = re.sub(r'\\s+\\([^)]*\\)$', '', home_team)  # Remove trailing parentheses\n                        \n                        if away_team and home_team and len(away_team) > 3 and len(home_team) > 3:\n                            # Generate PFR URL\n                            home_code = team_codes.get(home_team)\n                            away_code = team_codes.get(away_team)\n                            \n                            pfr_url = None\n                            if home_code and current_date:\n                                date_str = current_date.strftime('%Y%m%d')\n                                pfr_url = f\"https://www.pro-football-reference.com/boxscores/{date_str}0{home_code}.htm\"\n                            \n                            # Extract additional details\n                            day_match = re.search(r'\\((Thu|Mon|Tue|Wed|Fri|Sat|Sun)\\)', line)\n                            day_of_week = day_match.group(1) if day_match else None\n                            \n                            times = re.findall(r'\\d+:\\d+p', line)\n                            local_time = times[0] if times else None\n                            et_time = times[1] if len(times) > 1 else times[0] if times else None\n                            \n                            networks = ['NBC', 'CBS', 'FOX', 'ESPN', 'NFLN', 'Prime Video', 'Peacock', 'ESPN/ABC']\n                            tv_network = None\n                            for network in networks:\n                                if network in line:\n                                    tv_network = network\n                                    break\n                            \n                            games.append({\n                                'game_id': f\"{season_year}_{game_id:03d}\",\n                                'season': season_year,\n                                'week': current_week,\n                                'game_date': current_date,\n                                'day_of_week': day_of_week,\n                                'away_team': away_team,\n                                'home_team': home_team,\n                                'local_time': local_time,\n                                'et_time': et_time,\n                                'tv_network': tv_network,\n                                'pfr_url': pfr_url,\n                                'pfr_home_code': home_code,\n                                'pfr_away_code': away_code,\n                                'is_international': is_international\n                            })\n                            game_id += 1\n    \n    return pd.DataFrame(games)\n\n# Parse the full 2024 season\nprint(\"Parsing full 2024 NFL schedule with pdfplumber...\")\ngames_2024_full = parse_nfl_with_pdfplumber_full('NFL-Regular-season-2024.pdf', 2024)\n\nprint(f\"Found {len(games_2024_full)} games total\")\n\n# Check for international games\ninternational_games = games_2024_full[games_2024_full['is_international'] == True]\nprint(f\"International games (vs): {len(international_games)}\")\n\n# Check for any remaining URL issues\nmissing_urls = games_2024_full[games_2024_full['pfr_url'].isna()]\nprint(f\"Games with missing URLs: {len(missing_urls)}\")\n\nif len(missing_urls) > 0:\n    print(\"Games still missing URLs:\")\n    for _, game in missing_urls.iterrows():\n        print(f\"  {game['game_id']}: {game['away_team']} @ {game['home_team']}\")\n\nprint(\"\\nFirst 5 games:\")\nprint(games_2024_full[['game_id', 'week', 'game_date', 'away_team', 'home_team', 'is_international']].head(5))\n\nprint(\"\\nLast 5 games:\")\nprint(games_2024_full[['game_id', 'week', 'game_date', 'away_team', 'home_team', 'is_international']].tail(5))\n\n# Save to CSV for batch scraper\ncsv_filename = 'nfl_2024_schedule.csv'\ngames_2024_full.to_csv(csv_filename, index=False)\nprint(f\"\\nðŸ“„ Saved {len(games_2024_full)} games to {csv_filename} for batch scraper\")\n\n# Also save corrected version for auditing\ngames_2024_full.to_csv('nfl_2024_schedule_corrected.csv', index=False)\nprint(f\"ðŸ“„ Saved audit copy to nfl_2024_schedule_corrected.csv\")"
  },
  {
   "cell_type": "markdown",
   "id": "fcy8sqc9fgc",
   "metadata": {},
   "source": [
    "# NFL Player Stats Scraper\n",
    "\n",
    "Test scraping player statistics from Pro Football Reference box scores."
   ]
  },
  {
   "cell_type": "code",
   "id": "hvcqq49qlnc",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for testing single game scraping\n# This was used during development but no longer needed"
  },
  {
   "cell_type": "code",
   "id": "2pv99tdlpfe",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for parsing table structure\n# This was used during development but no longer needed"
  },
  {
   "cell_type": "code",
   "id": "62gxo3ragd5",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for finding advanced tables in HTML comments\n# This was used during development but no longer needed"
  },
  {
   "cell_type": "code",
   "id": "zzk2hkhk68o",
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport pandas as pd\nimport requests\nimport time\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n\ndef datafix_failed_games():\n    \"\"\"Scrape failed games and append to existing data files\"\"\"\n    \n    print(\"=== NFL DATAFIX SCRIPT ===\")\n    \n    # Manual corrections for rescheduled games\n    manual_fixes = {\n        '2024_225': {  # Browns @ Bengals - moved TO 12/22\n            'correct_date': '2024-12-22',\n            'correct_url': 'https://www.pro-football-reference.com/boxscores/202412220cin.htm'\n        },\n        '2024_235': {  # Broncos @ Chargers - moved TO 12/19\n            'correct_date': '2024-12-19',\n            'correct_url': 'https://www.pro-football-reference.com/boxscores/202412190sdg.htm'\n        },\n        '2024_247': {  # Colts @ Giants\n            'correct_date': '2024-12-29',\n            'correct_url': 'https://www.pro-football-reference.com/boxscores/202412290nyg.htm'\n        },\n        '2024_248': {  # Falcons @ Commanders  \n            'correct_date': '2024-12-29',\n            'correct_url': 'https://www.pro-football-reference.com/boxscores/202412290was.htm'\n        },\n        '2024_270': {  # Bengals @ Steelers\n            'correct_date': '2025-01-04', \n            'correct_url': 'https://www.pro-football-reference.com/boxscores/202501040pit.htm'\n        }\n    }\n    \n    # Read error log to get failed game IDs\n    with open('scraper_errors.log', 'r') as f:\n        error_lines = f.readlines()\n    \n    failed_game_ids = []\n    for line in error_lines:\n        match = re.search(r'(2024_\\d+)', line)\n        if match:\n            game_id = match.group(1)\n            if game_id not in failed_game_ids:\n                failed_game_ids.append(game_id)\n    \n    print(f\"Found {len(failed_game_ids)} failed games to retry\")\n    print(f\"Manual fixes available for: {list(manual_fixes.keys())}\")\n    \n    # Load updated schedule\n    schedule_df = pd.read_csv('nfl_2024_schedule.csv')\n    failed_games = schedule_df[schedule_df['game_id'].isin(failed_game_ids)].copy()\n    \n    # Apply manual fixes\n    for idx, row in failed_games.iterrows():\n        if row['game_id'] in manual_fixes:\n            fix = manual_fixes[row['game_id']]\n            failed_games.at[idx, 'game_date'] = fix['correct_date']\n            failed_games.at[idx, 'pfr_url'] = fix['correct_url']\n            print(f\"Applied manual fix for {row['game_id']}: {fix['correct_url']}\")\n    \n    # Skip future games (not played yet)\n    today = datetime.now().date()\n    playable_games = []\n    \n    for _, game in failed_games.iterrows():\n        game_date = datetime.strptime(game['game_date'], '%Y-%m-%d').date()\n        if game_date <= today:\n            playable_games.append(game)\n    \n    print(f\"Games that can be scraped (already played): {len(playable_games)}\")\n    \n    if not playable_games:\n        print(\"No games to scrape - all failed games are in the future\")\n        return\n    \n    # Load existing data files to append to\n    output_dir = \"nfl_2024_data\"\n    existing_tables = {}\n    table_files = {\n        'basic_offense': f'{output_dir}/2024_season_basic_offense.csv',\n        'advanced_passing': f'{output_dir}/2024_season_advanced_passing.csv',\n        'advanced_rushing': f'{output_dir}/2024_season_advanced_rushing.csv',\n        'advanced_receiving': f'{output_dir}/2024_season_advanced_receiving.csv'\n    }\n    \n    for table_name, filepath in table_files.items():\n        try:\n            existing_tables[table_name] = pd.read_csv(filepath)\n            print(f\"Loaded {table_name}: {len(existing_tables[table_name])} existing records\")\n        except FileNotFoundError:\n            print(f\"WARNING: {filepath} not found - will create new file\")\n            existing_tables[table_name] = pd.DataFrame()\n    \n    # Import the scraper function\n    from bs4 import Comment\n    \n    def scrape_game_tables_with_id(soup, game_id, game_url):\n        \"\"\"Extract all 4 tables and add game_id to each record\"\"\"\n        tables = {}\n        \n        # 1. Basic offense table  \n        basic_table = soup.find('table', {'id': 'player_offense'})\n        if basic_table:\n            basic_data = []\n            tbody = basic_table.find('tbody')\n            if tbody:\n                rows = tbody.find_all('tr')\n                for row in rows:\n                    if row.get('class') and ('thead' in str(row.get('class')) or 'over_header' in str(row.get('class'))):\n                        continue\n                    cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]\n                    if cells and cells[0] == 'Player':\n                        continue\n                    if len(cells) > 1 and cells[0]:\n                        row_data = [game_id] + cells\n                        basic_data.append(row_data)\n            \n            if basic_data:\n                columns = ['game_id', 'Player', 'Tm', 'Pass_Cmp', 'Pass_Att', 'Pass_Yds', 'Pass_TD', 'Pass_Int', \n                          'Pass_Sk', 'Pass_Sk_Yds', 'Pass_Lng', 'Pass_Rate', 'Rush_Att', 'Rush_Yds', \n                          'Rush_TD', 'Rush_Lng', 'Rec_Tgt', 'Rec_Rec', 'Rec_Yds', 'Rec_TD', 'Rec_Lng', \n                          'Fmb', 'FL']\n                while len(columns) < len(basic_data[0]):\n                    columns.append(f'Col_{len(columns)}')\n                tables['basic_offense'] = pd.DataFrame(basic_data, columns=columns[:len(basic_data[0])])\n        \n        # 2-4. Advanced tables from comments\n        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n        advanced_sections = {\n            'advanced_passing': 'Advanced Passing',\n            'advanced_rushing': 'Advanced Rushing', \n            'advanced_receiving': 'Advanced Receiving'\n        }\n        \n        for table_name, section_text in advanced_sections.items():\n            for comment in comments:\n                if section_text.lower() in comment.lower():\n                    comment_soup = BeautifulSoup(comment, 'html.parser')\n                    adv_table = comment_soup.find('table')\n                    \n                    if adv_table:\n                        headers = ['game_id']\n                        header_row = adv_table.find('thead') or adv_table.find('tr')\n                        if header_row:\n                            for th in header_row.find_all(['th', 'td']):\n                                headers.append(th.get_text().strip())\n                        \n                        data_rows = []\n                        rows = adv_table.find_all('tr')[1:]\n                        for row in rows:\n                            if row.get('class') and ('thead' in str(row.get('class')) or 'over_header' in str(row.get('class'))):\n                                continue\n                            cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]\n                            if cells and cells[0] == 'Player':\n                                continue\n                            if len(cells) > 1 and cells[0]:\n                                row_data = [game_id] + cells\n                                data_rows.append(row_data)\n                        \n                        if data_rows and headers:\n                            max_cols = max(len(headers), len(data_rows[0]) if data_rows else 0)\n                            while len(headers) < max_cols:\n                                headers.append(f'Col_{len(headers)}')\n                            tables[table_name] = pd.DataFrame(data_rows, columns=headers[:len(data_rows[0])])\n                    break\n        \n        return tables\n    \n    # Scrape the failed games\n    scraped_count = 0\n    for game in playable_games:\n        game_id = game['game_id']\n        url = game['pfr_url']\n        \n        print(f\"\\nScraping {game_id}: {game['away_team']} @ {game['home_team']}\")\n        print(f\"URL: {url}\")\n        \n        try:\n            response = requests.get(url)\n            \n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, 'html.parser')\n                game_tables = scrape_game_tables_with_id(soup, game_id, url)\n                \n                # Check if any data was found\n                total_records = sum(len(df) for df in game_tables.values() if not df.empty)\n                \n                if total_records == 0:\n                    print(f\"  WARNING: No data found (game may not have occurred)\")\n                else:\n                    # Append to existing tables\n                    for table_name, new_data in game_tables.items():\n                        if not new_data.empty:\n                            existing_tables[table_name] = pd.concat([existing_tables[table_name], new_data], ignore_index=True)\n                            print(f\"  Added {len(new_data)} records to {table_name}\")\n                \n                scraped_count += 1\n                print(f\"  SUCCESS\")\n                \n            else:\n                print(f\"  ERROR: HTTP {response.status_code}\")\n                \n        except Exception as e:\n            print(f\"  ERROR: {e}\")\n        \n        # Be respectful with delays\n        time.sleep(2)\n    \n    # Save updated files\n    print(f\"\\n=== SAVING UPDATED FILES ===\")\n    for table_name, df in existing_tables.items():\n        if not df.empty:\n            filepath = table_files[table_name]\n            df.to_csv(filepath, index=False)\n            print(f\"Saved {table_name}: {len(df)} total records -> {filepath}\")\n    \n    print(f\"\\nDatafix complete: {scraped_count} games processed\")\n\n# Datafix function preserved for future use\n# Run with: datafix_failed_games()"
  },
  {
   "cell_type": "markdown",
   "id": "0aanrnpozeq",
   "metadata": {},
   "source": [
    "# Batch NFL Stats Scraper\n",
    "\n",
    "Scrape all 272 games from 2024 season in random order with delays."
   ]
  },
  {
   "cell_type": "code",
   "id": "yqv8iqm44w",
   "metadata": {},
   "outputs": [],
   "source": "import random\nimport time\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\n\ndef scrape_game_tables_with_id(soup, game_id, game_url):\n    \"\"\"Extract all 4 tables and add game_id to each record\"\"\"\n    \n    tables = {}\n    \n    # 1. Basic offense table  \n    basic_table = soup.find('table', {'id': 'player_offense'})\n    if basic_table:\n        basic_data = []\n        tbody = basic_table.find('tbody')\n        if tbody:\n            rows = tbody.find_all('tr')\n            \n            for row in rows:\n                if row.get('class') and ('thead' in str(row.get('class')) or 'over_header' in str(row.get('class'))):\n                    continue\n                    \n                cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]\n                \n                if cells and cells[0] == 'Player':\n                    continue\n                    \n                if len(cells) > 1 and cells[0]:\n                    # Add game_id as first column\n                    row_data = [game_id] + cells\n                    basic_data.append(row_data)\n        \n        if basic_data:\n            columns = ['game_id', 'Player', 'Tm', 'Pass_Cmp', 'Pass_Att', 'Pass_Yds', 'Pass_TD', 'Pass_Int', \n                      'Pass_Sk', 'Pass_Sk_Yds', 'Pass_Lng', 'Pass_Rate', 'Rush_Att', 'Rush_Yds', \n                      'Rush_TD', 'Rush_Lng', 'Rec_Tgt', 'Rec_Rec', 'Rec_Yds', 'Rec_TD', 'Rec_Lng', \n                      'Fmb', 'FL']\n            while len(columns) < len(basic_data[0]):\n                columns.append(f'Col_{len(columns)}')\n            \n            tables['basic_offense'] = pd.DataFrame(basic_data, columns=columns[:len(basic_data[0])])\n    \n    # 2-4. Advanced tables from comments\n    from bs4 import Comment\n    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n    \n    advanced_sections = {\n        'advanced_passing': 'Advanced Passing',\n        'advanced_rushing': 'Advanced Rushing', \n        'advanced_receiving': 'Advanced Receiving'\n    }\n    \n    for table_name, section_text in advanced_sections.items():\n        for comment in comments:\n            if section_text.lower() in comment.lower():\n                comment_soup = BeautifulSoup(comment, 'html.parser')\n                adv_table = comment_soup.find('table')\n                \n                if adv_table:\n                    headers = ['game_id']  # Start with game_id\n                    header_row = adv_table.find('thead') or adv_table.find('tr')\n                    if header_row:\n                        for th in header_row.find_all(['th', 'td']):\n                            headers.append(th.get_text().strip())\n                    \n                    data_rows = []\n                    rows = adv_table.find_all('tr')[1:]\n                    \n                    for row in rows:\n                        if row.get('class') and ('thead' in str(row.get('class')) or 'over_header' in str(row.get('class'))):\n                            continue\n                            \n                        cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]\n                        \n                        if cells and cells[0] == 'Player':\n                            continue\n                            \n                        if len(cells) > 1 and cells[0]:\n                            # Add game_id as first column\n                            row_data = [game_id] + cells\n                            data_rows.append(row_data)\n                    \n                    if data_rows and headers:\n                        max_cols = max(len(headers), len(data_rows[0]) if data_rows else 0)\n                        while len(headers) < max_cols:\n                            headers.append(f'Col_{len(headers)}')\n                        \n                        tables[table_name] = pd.DataFrame(data_rows, columns=headers[:len(data_rows[0])])\n                break\n    \n    return tables\n\ndef log_error(message, log_file=\"scraper_errors.log\"):\n    \"\"\"Log errors to file with timestamp\"\"\"\n    with open(log_file, 'a') as f:\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        f.write(f\"[{timestamp}] {message}\\n\")\n\ndef format_time_remaining(seconds):\n    \"\"\"Format seconds into readable time string\"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    if hours > 0:\n        return f\"{hours}h {minutes}m\"\n    else:\n        return f\"{minutes}m\"\n\ndef load_nfl_facts():\n    \"\"\"Load NFL facts from facts.txt file\"\"\"\n    try:\n        with open('facts.txt', 'r') as f:\n            content = f.read()\n        \n        # Split by lines and filter out empty lines\n        facts = [line.strip() for line in content.split('\\n') if line.strip() and not line.strip().isdigit()]\n        return facts\n    except FileNotFoundError:\n        return [\"Did you know? The NFL was founded in 1920!\"]  # Fallback fact\n\ndef batch_scrape_season():\n    \"\"\"\n    WARNING: This will scrape ALL 272 games from 2024 season!\n    Estimated time: 2-3 hours with 15-45 second delays\n    \"\"\"\n    \n    print(\"BATCH NFL SCRAPER STARTING\")\n    print(\"This will take 2-3 hours to complete all 272 games\")\n    \n    # Clean up previous runs\n    files_to_remove = [\"scraper_errors.log\"] + [f for f in os.listdir('.') if f.startswith('clean_game_')]\n    for file in files_to_remove:\n        if os.path.exists(file):\n            os.remove(file)\n    \n    if os.path.exists(\"nfl_2024_data\"):\n        shutil.rmtree(\"nfl_2024_data\")\n        print(\"Cleaned up previous run data\")\n    \n    # Load NFL facts for entertainment\n    nfl_facts = load_nfl_facts()\n    \n    # Create output directory\n    output_dir = \"nfl_2024_data\"\n    os.makedirs(output_dir)\n    print(f\"Created output directory: {output_dir}\")\n    \n    # Initialize error log\n    log_error(\"=== BATCH SCRAPING SESSION STARTED ===\")\n    \n    start_time = datetime.now()\n    print(f\"\\nSCRAPING STARTED at {start_time.strftime('%H:%M:%S')}\")\n    \n    # Load schedule\n    schedule_df = pd.read_csv('nfl_2024_schedule.csv')\n    total_games = len(schedule_df)\n    print(f\"Loaded {total_games} games to scrape\")\n    \n    # Calculate time estimates\n    avg_delay = (15 + 45) / 2  # 30 seconds average\n    estimated_total_seconds = total_games * avg_delay\n    estimated_hours = estimated_total_seconds / 3600\n    estimated_end = start_time + timedelta(seconds=estimated_total_seconds)\n    \n    print(f\"Estimated completion: {estimated_end.strftime('%H:%M:%S')} ({estimated_hours:.1f} hours)\")\n    \n    # Randomize order\n    games_to_scrape = schedule_df.sample(frac=1).reset_index(drop=True)\n    print(\"Shuffled games randomly\")\n    \n    # Initialize master DataFrames\n    master_tables = {\n        'basic_offense': [],\n        'advanced_passing': [],\n        'advanced_rushing': [],\n        'advanced_receiving': []\n    }\n    \n    successful_scrapes = 0\n    failed_scrapes = 0\n    \n    for i, game in games_to_scrape.iterrows():\n        game_id = game['game_id']\n        url = game['pfr_url']\n        \n        # Progress calculation\n        progress_pct = (i / total_games) * 100\n        games_remaining = total_games - i - 1\n        elapsed = datetime.now() - start_time\n        \n        print(f\"\\n[{i+1}/{total_games}] ({progress_pct:.1f}%) Scraping {game_id}\")\n        print(f\"Game: {game['away_team']} @ {game['home_team']}\")\n        print(f\"URL: {url}\")\n        \n        try:\n            # Make request\n            response = requests.get(url)\n            \n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, 'html.parser')\n                game_tables = scrape_game_tables_with_id(soup, game_id, url)\n                \n                # Check if we got any data\n                total_players = sum(len(df) for df in game_tables.values() if not df.empty)\n                \n                if total_players == 0:\n                    # No data found - log as potential date/parsing issue\n                    error_msg = f\"NO DATA FOUND - {game_id} | {url} | Teams: {game['away_team']} @ {game['home_team']} | Date: {game.get('game_date', 'Unknown')}\"\n                    log_error(error_msg)\n                    print(f\"  WARNING: No player data found - logged to errors\")\n                else:\n                    # Accumulate results\n                    table_counts = {}\n                    for table_name, df in game_tables.items():\n                        if not df.empty:\n                            master_tables[table_name].append(df)\n                            table_counts[table_name] = len(df)\n                        else:\n                            table_counts[table_name] = 0\n                    \n                    # Display table counts - more concise\n                    total_records = sum(table_counts.values())\n                    print(f\"  Data: {total_records} total records ({', '.join([f'{k}: {v}' for k, v in table_counts.items()])})\")\n                \n                successful_scrapes += 1\n                success_rate = (successful_scrapes / (successful_scrapes + failed_scrapes)) * 100\n                print(f\"  SUCCESS ({successful_scrapes}/{successful_scrapes+failed_scrapes}) {success_rate:.1f}%\")\n                \n            elif response.status_code == 404:\n                # Try alternate URL with other team's code\n                print(f\"  404 on primary URL, trying alternate...\")\n                \n                # Create alternate URL with away team code - handle both string and datetime\n                if isinstance(game['game_date'], str):\n                    date_str = game['game_date'].replace('-', '')  # Convert 2024-12-08 to 20241208\n                else:\n                    date_str = game['game_date'].strftime('%Y%m%d') if pd.notnull(game.get('game_date')) else 'UNKNOWN'\n                \n                alt_url = f\"https://www.pro-football-reference.com/boxscores/{date_str}0{game['pfr_away_code']}.htm\"\n                print(f\"  Alt URL: {alt_url}\")\n                \n                alt_response = requests.get(alt_url)\n                \n                if alt_response.status_code == 200:\n                    print(f\"  SUCCESS with alternate URL\")\n                    soup = BeautifulSoup(alt_response.content, 'html.parser')\n                    game_tables = scrape_game_tables_with_id(soup, game_id, alt_url)\n                    \n                    # Check if we got any data\n                    total_players = sum(len(df) for df in game_tables.values() if not df.empty)\n                    \n                    if total_players == 0:\n                        error_msg = f\"NO DATA FOUND - {game_id} | {alt_url} | Teams: {game['away_team']} @ {game['home_team']} | Date: {game.get('game_date', 'Unknown')}\"\n                        log_error(error_msg)\n                        print(f\"  WARNING: No player data found - logged to errors\")\n                    else:\n                        # Accumulate results\n                        table_counts = {}\n                        for table_name, df in game_tables.items():\n                            if not df.empty:\n                                master_tables[table_name].append(df)\n                                table_counts[table_name] = len(df)\n                            else:\n                                table_counts[table_name] = 0\n                        \n                        # Display table counts - more concise\n                        total_records = sum(table_counts.values())\n                        print(f\"  Data: {total_records} total records ({', '.join([f'{k}: {v}' for k, v in table_counts.items()])})\")\n                    \n                    successful_scrapes += 1\n                    success_rate = (successful_scrapes / (successful_scrapes + failed_scrapes)) * 100\n                    print(f\"  SUCCESS ({successful_scrapes}/{successful_scrapes+failed_scrapes}) {success_rate:.1f}%\")\n                else:\n                    # Both URLs failed - log error\n                    error_msg = f\"404 BOTH URLs - {game_id} | Primary: {url} | Alternate: {alt_url} | Teams: {game['away_team']} @ {game['home_team']} | Date: {game.get('game_date', 'Unknown')}\"\n                    log_error(error_msg)\n                    print(f\"  ERROR: Both URLs 404'd - logged to errors\")\n                    failed_scrapes += 1\n                \n            else:\n                # Other HTTP error\n                error_msg = f\"HTTP {response.status_code} - {game_id} | {url}\"\n                log_error(error_msg)\n                print(f\"  ERROR: HTTP {response.status_code}\")\n                failed_scrapes += 1\n                \n        except Exception as e:\n            error_msg = f\"EXCEPTION - {game_id} | {url} | Error: {str(e)}\"\n            log_error(error_msg)\n            print(f\"  ERROR: {e}\")\n            failed_scrapes += 1\n        \n        # Calculate time remaining - simplified\n        if i > 0:\n            avg_time_per_game = elapsed.total_seconds() / (i + 1)\n            estimated_remaining_seconds = games_remaining * avg_time_per_game\n            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_seconds)\n            \n            print(f\"  ETA: {estimated_completion.strftime('%H:%M:%S')} ({format_time_remaining(estimated_remaining_seconds)} remaining)\")\n        \n        # Random delay between requests with live countdown and random NFL fact\n        if i < len(games_to_scrape) - 1:  # Don't delay after last game\n            delay = random.randint(15, 45)\n            \n            # Show random NFL fact with subtle formatting\n            random_fact = random.choice(nfl_facts)\n            print(f\"\\nNFL FACT: {random_fact}\\n\")\n            \n            for countdown in range(delay, 0, -1):\n                print(f\"\\r  Waiting {countdown} seconds...\", end=\"\", flush=True)\n                time.sleep(1)\n            print()  # New line after countdown\n    \n    # Combine and save all DataFrames\n    total_time = datetime.now() - start_time\n    print(f\"\\n=== COMBINING RESULTS ===\")\n    print(f\"Total scraping time: {str(total_time).split('.')[0]}\")\n    \n    final_tables = {}\n    \n    for table_name, df_list in master_tables.items():\n        if df_list:\n            combined_df = pd.concat(df_list, ignore_index=True)\n            final_tables[table_name] = combined_df\n            \n            # Save to protected subdirectory\n            filename = os.path.join(output_dir, f\"2024_season_{table_name}.csv\")\n            combined_df.to_csv(filename, index=False)\n            print(f\"SAVED: {table_name}: {len(combined_df)} records -> {filename}\")\n        else:\n            print(f\"ERROR: {table_name}: No data collected\")\n    \n    # Save backup copies with timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    backup_dir = os.path.join(output_dir, f\"backup_{timestamp}\")\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    for table_name, df in final_tables.items():\n        backup_filename = os.path.join(backup_dir, f\"2024_season_{table_name}_backup.csv\")\n        df.to_csv(backup_filename, index=False)\n    \n    print(f\"BACKUP: All files backed up to {backup_dir}\")\n    \n    # Final logging\n    final_msg = f\"SCRAPING COMPLETE - Success: {successful_scrapes} | Failed: {failed_scrapes} | Total time: {str(total_time).split('.')[0]}\"\n    log_error(final_msg)\n    \n    print(f\"\\nSCRAPING COMPLETE!\")\n    print(f\"Successful: {successful_scrapes}\")\n    print(f\"Failed: {failed_scrapes}\")\n    print(f\"Success rate: {successful_scrapes/(successful_scrapes+failed_scrapes)*100:.1f}%\")\n    print(f\"Total time: {str(total_time).split('.')[0]}\")\n    print(f\"Errors logged to: scraper_errors.log\")\n    print(f\"Data saved to: {output_dir}\")\n    \n    return final_tables\n\n# DANGER ZONE: Uncomment to run full season scrape\n# batch_scrape_season()"
  },
  {
   "cell_type": "code",
   "id": "ih9ugf8zook",
   "metadata": {},
   "outputs": [],
   "source": "# REMOVED: Debug code for parser troubleshooting\n# This was used during development but no longer needed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}